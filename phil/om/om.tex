\documentstyle[a4wide]{article}
\begin{document}

\title{The Origin of Mathematics}
\author{\small{\em Micha{\l}\ Walicki}
\thanks{In spite of his reservations and refusals, Eivind Kolflaath is virtually the
co-author of this paper. Without him, it wouldn't merely be more confused and unclear,
but would not be written at all.}}

%\date{\hfill{\small Bergen-Galway, 1994-95}}
\date{\hfill\small 1994-95}
\maketitle


\section{Introduction}
In every science one finds a hard seed of reality and beautiful flowers of 
logical imagination. The former, the origin is rooted in our intuition and 
common experience. As the origin, it precedes any later development and lies 
beyond it.

Differentiation of a culture is the process in which various areas of common experience are isolated and established as independent disciplines.
A new branch of science emerges when some area of specific objects or problems
 becomes identified by abstracting away aspects of experience (sometimes 
another science) which thereafter will be considered as falling outside the 
scope of this branch.
Establishing a new discipline marks a break with the earlier experience. 
Not only are some objects isolated from the rest of the world but they are 
also viewed in a new light. A new language emerges suited to the description 
of these objects; a language which will become more and more abstract and 
refined and less and less accessible to the uninitiated. The pre-scientific 
intuition is converted into expressions of the new language, into fundamental 
notions and definitions of the discipline. At the same time the discipline 
develops its own, internal standards and methods. Laying down a foundation 
amounts to drawing a line separating the questions which may be legitimately 
asked within the discipline from those which fall outside it. {\em Foundation marks
 a definitive break with the origin.} From now on this science will only 
occasionally refer to the world outside its scope. For the most it will 
develop according to its own logic and standards. A prominent illustration of
 this break is the fact that scientists in different branches develop 
different, discipline-specific, modes of thinking. These are related to the 
specific objects and results of the discipline. New results are produced and 
new objects defined which, for the most, refer only to the earlier results, 
and very seldom to the foundation, not to mention the origin. 

This immanent development may have various forms. It may be a mere refinement 
of the discipline's objects leading, sometimes, to new more specific, 
sub-branches (like the separation of formal logic or computer science from 
mathematics). It may lead to merging sciences which originated from different 
sources (like chemistry and physics). It may also involve earthquakes changing
 the very foundations and introducing new paradigms. But all these are 
phenomena occurring within a science and not affecting the origin. The origin,
 the experience which precedes and lies before (and hence beyond) a particular
 science and its foundation is a constant point of indirect reference. It is 
what gives a particular science continuity. What brought us from Aristotle to 
Copernicus, from Newton to Einstein is completely different from the usual 
experience of the stars and material things -- the experience which gave rise 
to astronomy and physics. Foundations may and do change but such changes 
hardly affect the identity of a science. Aristotelian speculations about dogs'
 and frogs' souls, Linn\'{e}'s classifications and molecular biology certainly 
rest on very different foundations. Yet we find it natural to call them all 
(or at least relate them all to) biology. The immutability of the origin is 
the simple consequence of the break introduced by the foundation. The 
foundation can at best reflect the pre-scientific intuition -- in itself it 
constitutes something qualitatively new and distinct from the origin.

It follows that the questions about origin and foundations should be kept 
clearly apart. I do not want to deny that there may be reciprocal relations in
 the way we view both questions. Attention to the questions about the origin 
may inspire to the formulations of novel results. A thorough knowledge of a 
discipline may provide one with concrete similes and useful analogies of its 
origin. Yet the two remain qualitatively different and independent. And, for 
the most, the philosophers concerned with the origins and the technicians 
concerned with the particulars stay each on their side of the borderline. 
Occasionally, philosophical arguments may disturb the work of scientists but 
in general all philosophical hullabaloo about science does not trouble the 
tranquillity of scientific research. Scientists seem to be much better off: 
in the rare cases when they jump over the fence, they easily find an 
appreciating audience dying to hear the first-hand news.

I should warn the readers of all news that this paper is concerned exclusively
 with the question about the origin of mathematics -- understood not in the
historical but in the individual sense. The appearance of a science at some
point of history indicates the emergence of new distinctions, and in this way
only reflects its origin from the individual experience. Investigating this
latter source, I hope to show that the objects studied abstractly in 
mathematics have their origin in the concreteness of the pre-scientific 
intuition and experience. We will look for these objects and, consequently,
no discussion of the 
foundations, no exchange of the arguments between formalists, intuitionists, 
platonists is to be expected. Reading the records of such discussions, one may
 sometimes find philosophically interesting ideas but in order to participate 
in them one should first turn into a mathematician. Otherwise, one may be 
easily confused by various announcement like, for instance, the one of von 
Neuman's: ``that this question [of the generally supposed absolute validity of
 classical mathematics], in and of itself philosophico-epistemological, is 
turning into a logico-mathematical one.'' Dividing mathematics into classical
 and non-classical involves one into a mathematical argument. Then one can 
only continue separating: geometry from arithmetic, analysis from algebra, 
universal algebra from linear algebra, and end up studying mathematics. But 
what is it that makes both classical mathematics and non-classical one equally
 mathematics? In spite of the foundational rumour within this discipline, this
 is not a mathematical question. Not only the whole mathematics will suffer no
 loss if it remains forever unanswered -- a mathematician asking this question
 is already on the other side of the fence. From the point of view of 
mathematics this question is as irrelevant as the question whether the 
continuum hypothesis is consistent with a particular axiomatization of set 
theory is irrelevant for a philosopher. Perhaps we could re-consider the 
ironic lesson of the Kantian lapse. That the introduction of non-Euclidean 
geometry, only a short time after publication of the first Critique, was able 
to demonstrate the total failure of apparently philosophical theory indicates 
that the philosopher was trying to do something more than philosophy. ``Love your neighbour, yet pull not down hedge.''

Putting aside all foundational discussions, I will begin by examining in 
section 2 some phenomenological assumptions and claiming that the origin of 
mathematics can be found in the experience of {\em pure distinction}.\footnote{When
writing this paper, I was not aware of the book ``{\em Laws of Form}'' by 
G.~Spencer Brown, nor the work following its publication in 1969. Its mathematical
contents seem to express the same intuitions as presented here.} Sections 3, 4 
and 5 will relate the notions of, respectively, point, (natural) number and 
infinity to the phenomenological origin, refining gradually the general 
assumptions from section 2. In section 6 I comment briefly on the notions of 
universality and necessity and their application to mathematics. Section 7 
concludes with an example of what I consider the relevance of mathematical 
experience.

\section{The first act}\label{se:act}
In our daily life we are surrounded by all kinds of objects which we can, more or less precisely, distinguish from each other. The table in front of me is obviously different from the chair on which I am sitting: they have different properties, occupy different regions of space, one can be moved without affecting the other, and so on. Saying that two things are different we are able to point out some different properties telling them apart and, of course, when we can find such properties we know that the things are different. The fact is so obvious that one has to invent quite a special name, like ``indiscernibility of identities'', in order to turn it into a respectable principle worth mentioning at all.

Now, it is usual to distinguish between a variety of impressions or properties and, on the other hand, a diversity of objects, things. The former has always perplexed the empirical mind since even it could not deny that some of these appearances seem to be manifestations of one and the same object. Ages of philosophical efforts have been spent on the attempts to figure out how the mind manages to construct a self-identical totality, an object, out of the diversity of impressions. In spite of that, it still remains unclear whether there exists an ontological difference between objects and their properties. As far as epistemology is concerned we can easily isolate a property and constitute it as an independent object of inquiry. Let us therefore ignore the (possible) difference and assume that whatever can be isolated can also be considered as an object. Then, we may say that the empiricist's language game begins with a diversity of objects given in experience. Does it really begin at the beginning?

Consider the vagueness  of our remote memories. The further we look into the past of our personal being the less we find there, the fewer definite objects and experiences. And it is not simply our memory which should be blamed. There {\em were} fewer objects and less diversity. It is only in the process of growing and education that we learn to distinguish things and experiences which were previously fused with an indistinct ``world background''. It takes time before a child learns that a chair and a table are two separate things. It takes time before it learns that a chair and a table are things at all, before they emerge from the indefinite background as two independent entities, that is, before they come to being and begin to exist. And when that happens it happens because they are distinguished from the background and from each other, because they emerge as distinct things.

Once we have become adults it is difficult to recall this original feeling of almost magical power of the surrounding which has not yet fallen apart, where parts have not yet been estranged from the background and acquired independent existence of their own. Perhaps, the ``aha-experiences'' may remind us of it when we suddenly solve a problem realising some crucial distinction or meaning which has evaded us so far. We can sometimes experience a similar situation when we are placed in an entirely new and unknown surroundings. We do recognise individual objects (this ability, once acquired, never gets lost) but the whole world appears chaotic, perhaps, meaningless. There are no indications as to which things or observations are significant, which mean something and carry relevant information and which do not. We experience a chaotic variety which -- due to the lack of meanings and significance -- appears as an undifferentiated, homogenous totality. Only after some time we are able to pull some objects out of this background, to distinguish the relevant from the irrelevant. 

Speaking phenomenologically, things are not simply there but rather arise from a formless homogeneity, from ``the dark and indistinct waters'' embracing everything before the creation of the world by the primordial act of distinguishing. I am not trying to defend naive ontology nor to confuse ontology and epistemology. It does not interest me in which domain we should place this process of distinguishing. It seems to me that it belongs to religion as much as to metaphysics but this is by no means crucial here. Important is not where it takes place but that it 
{\em does} take place.

So, the mystery is not how the mind forms, out of the diversity of perceptions and atomic properties, the idea of a sustaining, self-identical object but on the contrary, how from the original uniformity it passes to the multiplicity of independent individuals, how the One becomes Many. Fortunately, in the present context we do not have to spend much time arguing whether multiplicity comes first or whether it proceeds from some original unity. 
The issue belongs to metaphysics and I hope to be more concrete than that. No matter which view we adopt -- that the multiplicity of mutually distinct elements is the first which 
is given or else that objects appear only against a common background -- the fundamental 
fact is the same: we do distinguish and we have to distinguish before we can do anything 
else.

Let's clarify a bit the use of the word ``distinction''. I am not asking how we make distinctions. I am not inquiring into the criteria of identity nor the criteria of distinctness. Certainly, in practice we consider the properties of two objects before we conclude that they are distinct. But our central concept is that of distinguishing as such, of the {\em pure distinction}. 
It is not based on the process of abstraction in which ``we obtain from each object a more and more bloodless phantom. Finally we thus obtain from each object a something wholly deprived of content; But the something obtained from one object is different from the something obtained from another object - though it is not easy to say how... '' [1]. It is always difficult to say how when one arrives at the border of the intelligible world. But such a process of abstraction proceeds in the direction opposite to (at least) the order of significance. The pure ability to distinguish constitutes the condition of possibility of any actual distinction and as such precedes any actual distinction of objects based on the differences of content. If it was not so, how could we recognise different contents in the first place? In order to say ``this is a chair and that is a table'' we must be able to say ``this chair is distinct from that table'' -- we must be able to distinguish.

The contentless universality of this first act may be grasped easier by observing that it underlies equally all domains of our activity. To be a biologist one has to distinguish all kinds of biological concepts and entities, to be a carpenter one has to distinguish all kinds of materials and tools, to live in the world one has to distinguish various objects and feelings. Pure distinction has no content and is not specific to any particular domain because it is the very first act of being in the world. Without it we would not live in the world but, perhaps, in the One. And the One is beyond any knowledge.

The act of distinguishing comes first in the order of significance -- not in any real order. Probably, one should not even call it an ``act'' since it does not constitute any independent totality. In the actual experience we never make just a pure distinction. The latter occurs only together with some actual act of distinguishing. It occurs in the background of any actual act, so to speak, at the border of -- or even beyond -- the horizon of the actual objects being distinguished. This might tempt one to call it merely an aspect, or a formal property, of all acts. However, although an aspect of an act is not an act, an aspect of an experience may be an experience. Above all, the ground of an experience is also an experience. We do not have to go through the whole process of gradual abstraction in order to reach the insight that objects appear to us as distinct objects. Just like in any experience we also experience ourselves in the immediate self-consciousness as having the experience, so when distinguishing between some objects we also experience the fact of distinguishing. 
The latter will be called the {\em experience of pure distinction.}
Perhaps, this is an experience of another kind, of higher order, or whatever, but an experience it is nevertheless. I do not think that anything I have said so far can be termed otherwise as ``triviality''. Of course, we distinguish things; it is obvious. This is obviousness of an immediate experience. We know it -- one way or another but we obviously know it.

{\em Pure distinction} is the correlate of this experience. 
I will maintain that it is the counterpart of the fundamental mathematical notions and 
as such can be considered the origin of mathematics. Pure distinction
 may emerge
as an intentional object when the objects originally experienced as different,
have been abstracted away and the pure fact of their being different has
been moved into focus.
The experience
of mathematical objects is the experience of nothing but pure distinction. It
does not involve prior objects but posits the pure distinctions as its primary
and only objects.\footnote {Deciding whether (1) there is only one, Platonic 
pure distinction which is the correlate of each instance of its experience, or
else (2) there are many instances of pure distinction experienced in different 
cases, would not change anything in the following presentation. Even assuming
the multiple instances of pure distinctions (2), one is still unable
to see any difference between them and the possible general notion of pure 
distinction. Talking about trees, one may wonder whether there is something
common and more abstract than any tree, which makes them all instances
of an ideal tree. But there is nothing in a pure distinction which could be 
abstracted away yielding a more general notion unifying different instances.
In either case, one has to talk about pure distinction as if (1) was the case.}
But the experience of pure distinction remains outside -- and comes before -- the foundations and constructions of mathematics. 
True, establishing mathematics as a science, laying down the foundations,
requires, as usual, abstraction of its specific objects from the totality of
experience. The point is, however, that such an abstraction is not needed in
order to experience pure distinction.
With the reservation that it is an experience in its own right, one may take it as our counterpart of Kantian ``a priori forms of perception''.

I have different reasons to believe it to be ``the first'' experience but in the present context let me only draw your attention to the fact that mathematics -- abstract as it is -- is probably the eldest among the sciences. And in spite of the tremendous development it has not changed! Learning physics we never hear about the Ionic philosophy of nature or Aristotelian principles. But learning mathematics we still go through the theorems of Thales, Phytagoras, Euclid. 
Even if we go as far back as Egyptian engineering or Chaldean astrology we still find sound mathematical calculations. As the contributions to the mathematical knowledge they are as valid, relevant and {\em mathematical} as the theorems of Gauss, Banach or Skolem. Thus, unlike other sciences which have either gone through the processes of essential changes before reaching their modern form or else appeared only very recently, mathematical form has remained unchanged since the very beginning. All other sciences emerge as a consequence of extracting from the whole human experience some restricted domain -- of specific objects or problems. The notions of such a domain may then undergo a gradual abstraction which eventually yields quite abstract entities with which most modern sciences are occupied. The abstract character of a science is always the end result, never the beginning. 
But this schema obviously does not work for mathematics. Its original objects have not 
changed since its beginning. And if we try to elucidate the basic notions of point, number and the like by a reference to the process of abstraction we would have to explain also what made our ancestors so intensely interested in just this line of abstraction and made them ignore more or less all others -- why did they carry out this line of abstraction to its very extreme while in all other areas they stopped at a very elementary level? I do not think that anybody had to abstract himself toward the notion of a number by disregarding more and more properties of actual objects. 
If the {\em experience} of pure distinction lies in the background of our being in the world and, as I hope to show, of mathematics, then there is no need to
 make our ancestors so mystically different from us, because there is no need for abstraction at all.

Summarising briefly: distinguishing accompanies all possible acts -- not only as the condition of their possibility but also as an experience, immediate consciousness of distinguishing.

Furthermore, pure distinction concerns only the simple contentless fact of something being distinct from something else. Hence it is entirely irrelevant for the content and meaning of actual experience.

And finally, the point which we have merely touched upon but to which we will return very shortly: distinctions, whether pure or not, never come alone. We always distinguish something from something else. Even if we adopt the originality of the formless background, we still have to admit that once the One is dissolved it becomes Many, not just a new one against the original One.

\section{What is a point ?}\label{se:point}
A point is a pure difference.
%\footnote{Whether it actually {\em is} such a difference or, perhaps, only {\em represents} it, is an epistemological question which falls outside our interest in the origin. The intention is to illustrate that the relation of mathematical objects to the pre-scientific experience is not different than the relation of, say, the objects of physics. Whether we consider the objects studied by physics as representations, as abstractions, or else as symbols of some real objects, we can apply the same epistemological explanations to the basic objects of mathematics.}
It has no content, no properties -- it simply is and as such is different from non-being. It is the ideal atom, the least imaginable thing which still is; we cannot remove anything from a point without, at the same time, removing the point itself, that is, without erasing the difference. What is the difference between the infinite blank plane and the one with a point on it? Well, the former is blank and the latter contains a point. But what difference does the point make? Only that it {\em is}, i.e., no difference of content -- merely a pure difference. It is the difference between pure being and non-being, the binary yes or no, "1" or "0". A point indicates that there is something distinct from the background of the plane but the only thing establishing its presence is this pure distinctness from the background. On the infinite plane without any imagined, superimposed coordinate system there is no position -- it does not matter where we put the only point because there is no ``where''.

A single point does not appear as distinct from other points (which are not there yet) but only from the background. Every next point appearing on the plane, however, is distinct not only from this background but from the previous points as well. The pure distinction of the first point establishes the axis mundi, the ``where'', in relation to which the following points can appear. Thus it allows introduction of further distinctions. But this endangering concretization, emergence of relative positions and impure distinctions should not cause any confusion concerning the pure nature of a point. Any point may appear as the first one. As such any point is first of all a pure distinction which comes before the relativisation and comparison with other points.

We might try to stretch the argument saying that neither does multiplicity of points involve anything but pure distinctions. Imagine a very elementary activity of marking accidental points in the sand as a rather meaningless entertainment.
% with multiplying the number of distinct places marked. 
The observations of their relative positions, distances and, much later, of the pattern they create are secondary acts. Encountering arbitrarily placed points in a situation which does not suggests any intention hiding behind them, we would probably recognise merely some points -- pure distinctions. Considering the pattern in which they are arranged, distances between them and the like would be quite an advanced interpretation compared to the original impression.

I will return to this matter in the following sections. But let us begin the considerations of multiplicity in the natural context of numbers asking the next question. 

\section{Why do we count apples rather than apples and pears?}\label{se:apple}
Introducing us to the notions of number and counting our teacher started to put apples -- one after another -- on the table. ``We have one apple. What happens if I put another apple? Well, now we have two apples. And if I put yet another one? Well, . . . '' Did not your teacher do a similar thing? 

It bothered me a little what should happen if he run out of apples but he changed the focus before that happened and made me think of other things. Only much later I realised what would have happened. I understood why he did not suddenly pull out a pear and put it on the table instead of yet another apple. Can you imagine the confusion among the kids? An apple, yet another apple, more apples, a vague patter begins to emerge and, suddenly, a pear!! Not that the kids would for ever lose the chance of acquiring the concept of number but how much extra work for the teacher! How would he proceed to explain now that the fact that a pear is not an apple does not matter at all? How to explain that a pear is simply yet another object -- a fruit, perhaps -- distinct from all previous ones? An apple is so much an apple that the sixth apple put on the table is the same as the fifth one -- except that it is the sixth. A pear placed after the fifth apple would not be the sixth -- it is too different from the apples. It would be the first pear rather than the sixth fruit. The difference of content would come into the way of explaining the pure difference of number.

We do usually count apples separately from pears. And if we count both we say we are counting fruits. Thus Frege says that ``number is the extension of a concept'' because as soon as we count quite different objects together we seem to subsume them under some common, more general concept. ``In fact, we do not ask `How many are Caesar and Pompey and London and Edinburgh?''' 
In fact, we do not -- but we may. And counting cities is no different from counting cities and persons, counting fruits is no different from counting fruits and houses and the nasty persons one met last week. 
Number expresses not so much a ``property of a concept''\footnote{To be 
unnecessarily  precise, we should perhaps rephrase it as ``number is 
something like a property of a concept''.} 
as the unlimited ability to ignore the differences of content. Eventually, 
we count somethings or, to use a synonym, objects. The word ``object'' expresses
 exactly this pure self-identity, empty identity of a noumenal x which is itself
 merely because it is not something else. Frege's ``bloodless phantom'' is an 
object -- a hardly imaginable site of the ultimate identity of the thing he 
started with.\footnote{This fundamental importance of distinction is well illustrated by the logicist's definition of numbers [2]. For example, for number 2, one begins by stating that at least two objects fall under a concept F: 
$2_m(f) \iff \exists x\ \exists y : x\not = y\land f(x)\land f(y)$. Then, 
number 2 is said to apply to a concept F iff: $2_m(f) \land \neg 3_m(f).$ Identity (or dually, its negation) needed in the first part is the undefinable primitive relation of the logical language.}

If one wants to argue that apples on the table are not meant as an analogy of pure difference because they have different positions, appear on the table at different times, and so on, that is, because they fall within the extension of a concept where other differences are needed to distinguish between the objects, then I would ask why the teachers do not count fruits but only apples. The argument does not change the teacher's procedure which is: make the difference as small as physically possible, make the objects so similar that removing this last amount of difference would erase the distinction itself. If one feels a need for it, one might define pure distinction as such a smallest possible difference. 

Such a definition would still allow one to entertain very different opinions on the nature of this smallest difference but I do not believe a discussion about this nature to be fruitful. Hunting for the ultimate particles, for the eventual atoms is a process which, as Kant teaches, has no empirical limit. Neither does arguing about the smallest difference have such a limit because argument requires a definition, that is, it requires us to endow the object of discussion with some properties. A point cannot be the smallest and have content at the same time because any content can be reduced -- even if only by abstraction. One always tends to endow the smallest difference with some properties and it helps little to call these properties ``formal'' or ``pure forms''. If one agrees that pure distinction is apriori then any attempt at such a definition must be futile. Or, in a dual formulation of Frege's: ``Since any definition is an identity, identity itself cannot be defined''.
The pure distinction is a difference without reason, a difference between something and something which are identical in all respects -- except for the fact that they are different. Interesting are not the criteria but the fact. 

Empirically it may be impossible to produce such a difference without endowing the objects with some different contents. It may be hard to illustrate it using a multiplicity of points which must have different positions in order to be distinct. The fact that pure distinction never comes alone is captured by the notion of number or, more precisely, the fact that numbers occur all at once. Number does involve a collection or multiplicity as one has always been so anxious to emphasize.\footnote{I am not confusing multiplicity with number and I do not think that many philosophers ever did. Multiplicity belongs to the origin while a possible definition of numbers already to the foundations.}
What is essential, however, is that it is a multiplicity of indiscernibles -- every ``next'' element is identical to all the ``previous'' ones except that it is distinct. This is very naturally indicated in the unary notation for numbers: I, II, III, ... , or even better: $\bullet, \bullet\bullet, \bullet\bullet\bullet, ...$
This however still reminds us of a sequence, i.e., a system, where some additional relations (like this of being a successor) obtain between various elements. And in fact, as P.Benacerraf argued in [3], what makes a number into a number is not any mystical quality but its relation to other numbers. There is no single number -- only a number system. Although I am not dealing with the foundations, I should probably go a small step further and illustrate the way in which the original intuition is extended and preserved in our number system.

Psychologists maintain that human being is capable of simultaneously perceiving 
a difference between at most 7-10 objects, that is, intuitively there should not
 be more numbers than that. Arbitrary large numbers are not given intuitively --
 I doubt that anybody, no matter how autistic, would be able to tell apart the 
collections of 23 004 and 23 005 pairs of shoes. And in fact, from the point of 
view of the origin, the number system with only two constants: -- 1, many -- 
contains already all essential features. (Anthropologists, for instance, 
announced the discovery of tribes using the system with just four numbers: 1, 2,
 3, many.) The rest of our (and any other) number system is a creative invention
 of the mathematical mind. It is only a more or less sophisticated extension of 
the original intuition. Consider the Roman system. Romans had enough symbols to 
count their soldiers and calculate taxes. It is easy to imagine what they would 
have done when faced with still bigger numbers, perhaps, the number of atoms in 
the universe. They would have to invent a new symbol because in the absence of a
 coding scheme (like in the positional notation) only sufficiently many symbols 
guarantee that one may express sufficiently many distinctions. They might, for 
instance, introduce the symbols ``\#'', ``@'', ``$\aleph$''. 
Then -- and only then -- they might order them, 
as $\# < @ < \aleph$, and continue doing their mathematics. True, the inconvenience of 
such a notation was probably responsible for the lack of development of more 
advanced mathematical ideas in the ancient Rome. Nevertheless, it was a 
mathematical notation and Roman mathematics, no matter how primitive, was a 
mathematics.

We handle arbitrary large finite numbers with the help of the representation of their successive generation and the convenience of the positional notation. We recognise immediately  23005 as bigger than 23004 by comparing the relevant digits. But first we have to recognise them as distinct numbers. Numbers express only pure distinctions, namely mutual pure distinctions. The point $\bullet$
is distinct from the lack of a point, but the points $\bullet\bullet$ are no longer distinct from the lack of two points but from each other. All the points 
$\bullet\bullet\bullet$ are mutually distinct, and so on. Adding a new point amounts to indicating that there is something new which is simply distinct from everything else. 

Now, comparison of two distinct numbers amounts to a comparison of two multiplicities and is like an act of second order. But to do this requires a multiplicity in the first place. To the origin belongs only a single multiplicity -- relations between different multiplicities provide already a foundation for mathematical arguments. A trace of this origin may be found in the theory of ordinal and cardinal numbers. The consecutive numbers from 1 up to 4 are, in a sense, included in 4. We may consider them as mere names for the distinct objects 
$\bullet\bullet\bullet\bullet$. Counting the objects a, b, c, d we do not associate them with the increasing multiplicities as  
\begin{center}
\input{co1.tex}
\end{center}
The former would say that a, b, c, d are 10 and not 4 objects. In the latter case the dots across the dashed line correspond to the consecutive numbers 1, 2, 3, 4. This fact, that numbers do not depend on each other but express only mutual differences, is reflected in the definition of (cardinal) numbers requiring merely the existence of a one-to-one correspondence. Such a correspondence either exists (like between a, b, c, d and $\bullet\bullet\bullet\bullet$) or not (like between a, b, c and  $\bullet\bullet\bullet\bullet$). The fact is less transparent (though still present) in the definition of an (ordinal) number a as the union of all preceding numbers, i.e., as the set $\{ x : x<a \}.$  

The binary notation is in this respect equally truthful as the unary notation: the sequence 000, 001, 010, 011 does not (at least in the mind of an untrained person) create any definite association with an ordering, and it by no means indicates what the natural ``next'' element should be. The binary notation reflects very well the nature of pure difference in that it is based on a pure difference between the two basic symbols which accidentally are the number symbols "0" and "1" (but might as well be "$+$" and "$\bullet$" or, as in the computers, "on" and "off"). It is an excellent expression of the mutual pure differences: each two among the objects (numbers?) $+++, ++\bullet, +\bullet+, +\bullet\bullet$ are merely distinct by being purely different at least at one place.

Seeing, on the other hand, the sequence 0, 1, 2, 3 we cannot resist the impression that the next element will be 4. We are so accustomed to this notation that recognising the implicit ordering is a normal habit. But cardinal numbers, or even the binary notation, illustrate that the ordering is a subsequent act depending on the presence of distinctions. We may introduce arbitrary ordering among the differences, for instance: $+++ < ++\bullet< +\bullet+ < +\bullet\bullet$, or: 
$++\bullet < +++ < +\bullet+ < +\bullet\bullet$, or whatever you like. The fact that we do work with a system of (ordered) numbers reflects only the fact that there is nothing interesting to say about pure difference as such. It is or is not and that is all. With it alone our mathematics would be even more boring than the Roman one was. But even the fundamental arithmetic relation $<$ expresses pure distinction, namely, in so far as it amounts to the negation of identity. Two numbers are either identical or one is greater than the other. And whenever none of two numbers is greater than the other, the two are identical. We may start with arbitrary symbols and, once we have defined this relation, do all the rest of arithmetic. But of course, the relation is not a pure distinction; it is transitive and antisymmetric. It specialises the pure difference and thus brings us away from the origin toward the foundation.

Let us not be misled by the order of this exposition. A point was a very good image of a pure distinction emerging from the undifferentiated, formless background. But no distinction occurs alone, we never have only the indistinct background and only one single object extracted from it as might have been suggested by the picture of the plane and a point. As soon as there appears one point, we know that all other points are there too. The idea of number 1 is accompanied by the (immediate or mediate) consciousness of the presence of all (at least natural) numbers. One does not have to generate them. In that lies the static character of the mathematical universe. It is hard to see what -- except for the empirical bias, some technical difficulties or even theoretical prejudice -- leads some people to couple numbers with time. There is no becoming in the mathematical universe even if any number, in order to be used, must be constructed (as the intuitionists would like it) or schematised (as Kant would say). We may imagine the natural numbers being generated one after another in a sequence and, technically, we may use the construction of real numbers as limits of sequences of the rational ones. But such a practice does not imply that we should reject the intuition of the static universe of numbers, the intuition that the practice merely reflects the ``recognition of a relation that was there already''. 

The static character of the mutual pure distinctions is well reflected in the intuition of the static universe of numbers. But the same applies to geometry. As soon as one recognises a point on the plane, the plane acquires a dual character: either of an undifferentiated continuum or else of a dense, infinite collection of points. Because it is hard to get rid of the visual image of a point, it is also so easy to assume that there may be only one point -- just mark it on a piece of paper. But any actual point is not a point -- it has an extension, a content. If we want to form a better representation of a point and, especially, of the necessary multiplicity of points, we should consider the fact that one may put the point anywhere. Also the next point may be put anywhere, and so on. Marking a point on the plane is not so much creating it as merely indicating that it is there, or that we want to focus on the fact that it is there. Having marked a point we can not fail to see that the mark merely indicates -- the point is there whether we mark it or not. And so do all other points.

The duality forms, as Brouwer put it, two-oneness. After the distinction has occurred it becomes perplexing to decide whether continuum consists of parts or not, whether things are infinitely divisible or not, whether One is a collection of Many or else Many is really One. From our phenomenological perspective it should not be necessary to review Kantian antinomies nor modern theories of mathematical infinity and continuum. Let us conclude this section with the next triviality: there is not a single number but a whole, a totality of numbers and this reflects the fact that there is no single (pure) distinction -- no thing is given in isolation. At the moment when we recognise and see the chair we see the table too. And even if we do not see the table, we know that it is there. 

\section{What is the axiom of infinity good for ?}\label{se:infinity}
A pure distinction does not occur in isolation but always involves a multiplicity of distinctions, namely, mutual pure distinctions. This mutuality, and not the ordering, is the phenomenological basis of numbers. Nevertheless, the question ``is there the biggest number?'' is almost as natural as ``is there any limit to the distinctions we make?'' or, perhaps, ``are there only finitely many things in the universe?''. The goal of this section is to illustrate that the experience of infinity is as fundamental as that of pure distinction. Worries about the biggest number arise not from the intuition of the ordering of the numbers but from the immediate intuition of the limitless.

A discussion of the variety or reality of the mathematical notions of infinity would involve us into unnecessary technicalities. The only relevant instruction we may receive from the foundations of mathematics is that infinity must be introduced by means of an axiom. Starting with numbers only, although it may involve us into infinite progression, will never give us an actual infinity. If a set theoretician wants to speak about infinite numbers (or sets) he has to grant himself the luxury of the dogmatic statement ``there exists an infinite set''. Besides the axiom ``there exists an empty set'' this is the only existential postulate needed for modelling the mathematics within the (Zermelo-Fraenkel) set theory. We start with the empty set, \O, introduce a pure distinction (which in the language of set theory is a pair of parentheses "$\{$" and "$\}$"), and proceed as long as we realise that there is no other way to reflect the intuition that ``there should be an infinite collection'' than to simply admit its existence.

Mathematics operates with a whole hierarchy of infinities, it multiplies them ad infinitum searching for the ultimate universe embracing all imaginable (mathematical) entities. The resulting paradoxes are eventually solved by admitting that the ultimate totality cannot be a mathematical object in the same sense as all the objects it contains. All such considerations will not disturb us here because phenomenologically there is only one infinity. The fact that formalisation of this original intuition leads to ``discovery'' of new unexpected objects does not mean that all these consequences of the formal development can be projected back onto the origin. At most, they affect the foundations.

In fact, the experience of infinity is slightly different from its mathematical
formalisation. It is given in the immediate, though not necessarily immediately
conceptualised, intuition. It is the experience of an overflow, of an excess, 
of transcendence.
In this way, it is an experience of the One, of the
homogenous ``Beyond'' preceding and transcending any differentiation. If you like, it
is the immediate self-consciousness, that is, consciousness of our own finitude which 
recognises itself as such only because it is confronted with the ultimate ``Beyond''.
%(Lying beyond any distinctions, its conceptual content amounts to {\em nothingness}. But choosing this characterisation we should not forget that it is an experienced nothingness, and not just a non-existent emptiness.)

This homogenous continuity, the experience of the limitless, is the constantly
present background which is not annihilated by the arising distinctions. 
Its constant presence reminds only that distinctions have no limit, that one can always proceed and introduce
further distinctions. However, as this original limitless is pushed into
the background by the distinctions, its experience acquires a new character. 
It becomes a differentiated infinity or chaos. 
In concrete terms, it becomes the experience of the 
objective -- independent -- existence of the world of things {\em with} all its 
richness, variety and unpredictability exceeding all my powers, of there being 
something more than what I can manage to appropriate and control.  
In  more philosophical terms it means: there are always more objects, more distinctions made than I am able to comprehend and incorporate into a body of knowledge. In this sense, it is an experience of differentiated infinity. 
Thus, it seems to be much more tractable than the more abstract experience of 
pure distinction. 
If we call it ``objectivity of the world'' or the ``ultimate transcendence'' then only 
some philosophers could follow us. But if we consider it as the unpredictable richness, 
power and variety -- and consequently, uncontrollability -- of the world, the understanding audience becomes at once quite numerous.

This duality of the experienced infinity is unavoidable.
Nothingness of the undifferentiated background is our constant companion, the
shadow of our finitude. But
once we begin to distinguish, we are forced to think in terms of distinctions. 
If we think of places or times we have never experienced, we immediately think
of them in terms of things, situations, qualities, moods, etc. which we imagine
could be encountered there. And when we try to {\em imagine} what possibly is
``beyond'' all our limits, we imagine it as a differentiated totality, as 
something which would be distinguished. And, obviously, once we arrive to a 
place we have never visited before, we see different things. But the shadow is
still there.

According to the philosophy of Many the chaos of atoms, sense perceptions, impressions, or whatever, is the first principle. In the philosophy of the One, chaos should be the first modification of the indistinct background in the process of the emanation of forms and the world. Turning this view upside down one may say that chaos -- the infinity of pure distinctions -- is the closest possible image of the One available to us staying below, in the midst of distinctions. Chaos is the reflection of the origin in the world of distinctions. 

The duality in the experience of infinity was traditionally reflected in the 
distinction between actual and potential infinity. Actual infinity corresponds to
the contentless experience of our finitude, of the undifferentiated ``Beyond''.
Potential infinity reflects the superposition of distinctions onto this background, the intuition that no matter how many distinctions have been made, it is
always possible to make more.

It is false that there can be only a single point. But neither can there be only a limited multiplicity of points. Appearance of a point is instantaneously accompanied by the appearance of the infinity of points. We have actually gone a roundabout way to reach this triviality. We started with a point on the plane while the plane itself is the first image of infinity. But while the plane is
the image of actual infinity, the infinity of points is its differentiated reflection. The plane is One, a homogenous continuity but, as soon as we make the first distinction of a point, it ceases to be one and becomes Many, a dense 
collection of the potential infinity of points.

The intimate relation between multiplicity and infinity is equally fundamental in the case of numbers. They not only do not occur in isolation or, as formalisation would like it, successively one after another -- they are also given all
at once. If we wanted to use numbers to illustrate, equally precisely as in the
case of plane, the duality of the experience of infinity, we would probably
need to refer to the modern mathematics. Having admitted actual 
infinities, it constructed the distinction between continuum and countability,
and then run into the arguments with intuitionists.
However, also with respect to the natural numbers, the duality has been 
observed for long time as
the distinction between actual and potential infinity.
Refutation of the former makes things (and intuitionism!) counterintuitive
because, as far as the experience is concerned,
infinity accompanies necessarily pure distinction -- this is the obvious meaning of the question ``what is the biggest number?'' Although one naturally applies the argument from succession to justify the answer that ``there is none'', the answer itself needs no such justification. I am by no means after the induction principle but want to emphasize that it occurs post factum, when number and infinity already are given. 

We cannot prove the existence of actual infinity, just like we cannot prove the existence of infinite sets. Set theoreticians realised it very quickly and added the axiom. It took a bit longer for the philosophers to realise that they cannot produce any conclusive argument for the existence of the external world but they, too, seem to be arriving at a similar conclusion. Just like infinity lies obviously beyond the realm of natural numbers and cannot be reached by a mere progress, so does the experienced infinity lie beyond our powers to embrace things in the schemata of finite distinctions and concepts. And just like there is something odd in the neurotic attempts to convince oneself (and others) that everything is perfectly clear and under control, just like there is something implausible in the attempts to explain that the world out there is really only in here, so there is something, at least mathematically, counterintuitive in the denial of the statement ``there exists an infinite set``. We may occasionally take recourse to the universe of merely finite numbers but only for purely technical reasons. The argument for the addition of the infinity axiom -- ``it seems natural to admit infinite sets'' -- is the most honest and respectable I can imagine.

\section{Universality and necessity of mathematical truths}\label{se:universal}
The exposition so far should have made it clear why mathematical truths are universal. In so far as they reflect their origin they are ``synthetic a priori''. There would be some major changes in the table of categories, in the transcendental aesthetics and deduction, but the line of the argument remains exactly the same. The unity of the triad pure distinction - multiplicity - infinity constitutes the basis of all our experience, knowledge and activity. As soon as we distinguish something from the formless background, the whole variety of things emerges and so does the horizon, the limit beyond which there are always more distinctions remaining, however, inaccessible to us because of the finitude of our mind. As soon as we begin to introduce distinctions in the world, the points and the numbers are there, and so is the intuition of infinity. Hence mathematics is truly universal: not because it can say something about the content of any experience but because it does not say anything about such a content -- only that each content must be distinguished. Obviously, the universality is extremely poor -- it says the same about everything, and only as much as is related to the pure fact of distinguishing. 

It might seem that universality accounts also for necessity, that, as Kant meant, ``the two are inseparable''. But they are not only separable but very different. Let me first remark that I am not concerned with the ontological status of necessary judgements. I do not want to argue for nor against determinism, I only want to observe some differences in the meanings of the two notions.

Universality will say ``something is always valid'', necessity ``something can not be otherwise''. The former is quite a natural concept. If it is empirical, then it is exactly what makes it natural. To some extent everybody makes generalisations and arrives at some universal formulations. Now, one may say ``all ashtrays in this room are green'' but we should not confuse the syntactic form of a judgement (the mere presence of the universal quantifier) with the universality of the judgement. Universality involves generality and is concerned with the totality of the actual world, the world we live in. What has been said so far is a universal statement: we always distinguish -- so it is. But such ``so it is'' is not sufficient for necessity because necessity is concerned not only with the actual world but with all possible worlds. It cannot merely say what is the case, it also has to exclude its opposite from all possible worlds. Only by confusing the universal quantification over the objects within the world with the universal quantification over the possibilities of an object we may confuse universality with necessity. 

But most possible worlds are completely irrelevant and this is what makes necessity an ``unnatural'', almost inhuman property. Implausibility of claiming that something could not be otherwise is also the reason for the almost instinctive rejection -- at least by the common sense -- of all sophisticated arguments produced in favour of determinism I have said that mathematics is universal because it relates to the way in which we experience any content: by first distinguishing it. But what could it mean if I said that mathematics is necessary? That we can not do without it or that we can not do it otherwise? Well, we certainly can do without mathematics and as to ``doing it otherwise'' I only know that we do not, but not that we {\em could} not. 

Since universality is concerned with the actual world and necessity with all possibilities, the former does not imply the latter. But neither does necessity imply universality and this is due to the fact that the latter characterises judgements, knowledge, while the former objects. We do say that something applies to a general class of cases when we utter an universal statement but universality is here only a property of the judgement. We will, for instance, distinguish persons with a broad, general knowledge from those occupied only with their own particular world. Necessity, on the other hand, is {\em de re} -- it is a property of objects and state of affairs. Saying ``this statement is necessary'' we mean ``what it claims holds with necessity''. It is the behaviour of objects or some state of affairs which is characterised as necessary. Universal knowledge is only universal knowledge while necessary knowledge is knowledge of something necessary. After Hume's criticism it seemed impossible to maintain this understanding of necessary truths. But the idea of necessarily true knowledge was too seductive to be abandoned. The search for its site resulted in the notion of analytical judgements -- the judgements which are void of all real content and are true for purely linguistic reasons. I agree that necessity implies removal of the actual content but not that it is a purely linguistic phenomenon. It is not related to the actual objects but to the very special ones, namely, the contentless objects of mathematics.

Universality involves not only  ``for all x'' but also a kind of generality. Necessity, being related to the objects and not to the knowledge, does not require any generality. Necessary (judgements, if you still insist) say simply that ``it has to be the case that...'', ``it must be so that ...''. ``In the experiment which started at the Ridiculous Labs, CA, USA, on the 26th February, at 
14:03':52'':18''', 
the seventh particle which was generated had to turn left, the particle no. 21 had to make a U-turn and they had to anninhilate.''  Without making any claims to the physical plausibility of this statement -- it says that something is (was) necessary. It says that no matter what, given the above conditions things could not have happened otherwise. But one could hardly call it a universal statement. In fact, it is by designating more and more specific conditions, by isolating a situation or an object and excluding the possibility of interference from the unpredictable surroundings that we arrive at the laws which we consider necessary. The way to necessity goes via increased precision and specialisation, i.e., in the opposite direction than the way to universality. 

Thus the more content, the less necessity. The richer the perception of a situation, the more possibilities it unveils, the less tractable and the more difficult to control it becomes. And hence the attempts to design a grand theory of everything, to subsume the whole world under the rule of necessary laws impoverish the world. Certainly, some parts of the world can be reduced to simple entities which are prone to the descriptions in terms of the necessary and hence to be controlled. But the dangerous impoverishment occurs when the drive is uninhibited, when it is the drive to defeat everything escaping control. Only disappearance of content makes perfect necessity possible. 

So far, we are in agreement with the analytical theory. However, the necessary judgements are not empty tautologies valid for purely linguistic reasons but are concerned with the contentless objects of mathematics. The formula 
``$\forall x: A(x) \Rightarrow (P(x) \lor \neg P(x))$'' 
is a statement of formal logic while ``every ant is either parasitic or not'' certainly is not. According to the analycists the meaning of both formulations is the same vacuous condition of their necessary truth. Both are equally analytic. We need not quote any many-valued logic nor the intuitionistic denial of the principle of excluded middle. We only ask: why is the former formula mathematical and the latter is not? What is the difference between a contentless statement about an x and a contentless statement about ants? For the first, it is not true that the latter statement has no content. It has a content, a meaning which is different from the content of ``every ant is either a millionaire or not''. Similarly, the first statement has a content: it states the bivalence of P with respect to the objects to which A applies. Even if both might be verified by a purely linguistic analysis (which is not the case, for P may be fuzzy, many valued, and so may be ``parasitic''), their meaning is entirely different. More importantly, the difference cannot consist in that x is a symbol and the first formula merely a form of a statement, while ``ant'' is a word referring to something, because both statements are considered equally empty. The difference comes from the fact that x refers to something different than ``ant'' does. True, x may range over ants. But then ants cease to be ants and turn into empty mathematical objects. If we consider the latter formula an instance of the former, and justify it by the reference to the former, then we have changed its meaning and made it into a mathematical statement. Such a statement can be necessary because it describes contentless objects -- any other statement can be so only in so far as it does the same.

It may also be worth noticing that there are degrees of (empirical) content, and hence degrees of necessity -- objects may be more or less abstract, while propositions are supposed to be either analytic or not. Consider the increase of real content in passing from mathematics to physics, from biology to history, from sociology to literature. This increase is clearly accompanied by the decreasing degree of necessary determinations or, if you allow me, by the increase of freedom. It is no coincidence that the scientific and philosophical attempts to establish a system of necessary laws end up with abstract statements. But the statements are abstract not because they are empty tautologies -- they are abstract because they had to dispense with most of the actual content of the described objects. The search for the infallible laws leads sciences to construe their objects in a more and more simple and elementary fashion because the search for necessity requires precision, that is, reduction of the real content. Unfortunately, this applies not only to quantum mechanics but to most sciences or, at least, what wants to call itself a science. Not only natural sciences but also economy, sociology, even psychology display the symptoms of the mathematical disease. The mathematical point, the vanishing (or rather the barely appearing) indication of something-being-there is the perfect atom, the constant ideal of the knights of necessity.

Mathematical statements are not empty tautologies. Perhaps, one may develop an apparatus making $1\not = 2$ an empty statement. I, for my part, can not imagine how one could even start doing this without the prior {\em experience} of the distinction between one $\bullet$ 
and another $\bullet$, without the {\em experience} of $\not =$.
 Mathematical propositions tell us the story of the objects they describe. Their necessity follows not from their emptiness but from the emptiness of these object. It does not hide in any formal properties of the proof techniques or particular axiomatizations. All such techniques are equally necessary because they all have to conform to the standards of non-ambiguity set up by their objects. The fact that a mathematical theorem is either true or false mimics only the contentless duality of the pure distinction. (And no many-valued logic can change anything in this respect. Such a logic is based on multiplicity rather than duality of truth values and, more importantly, (meta)theorems about such logic are usual mathematical theorems which either hold or not.) 
Thus it is the ultimate poverty of mathematical objects which accounts for their necessity. Should it be the ideal of any other science? Why does it exercise such an attractive power on our mind which, certainly, is occupied with something more than the bare alternative of yes or no, "1" or "0", being or non-being?

\section{Relevance and irrelevance}\label{se:relevant}
Mathematical truths achieve the highest degree of necessity due to their objects being completely void. Hugo Steinhaus said that ``mathematics is the study of objects which do not exist''. The discussion whether, or in what sense, there exist objects without any content and properties most probably would not make anybody wiser. But if this is what Steinhaus meant by ``do not exist'' I can gladly accept his definition. Now, pure distinction, although an experience in its own right, is not self-subsistent and occurs only as one aspect -- the deepest but still only one -- of any act. Being completely void of content it is obviously irrelevant for any actual situation and experience. Similarly, mathematics must be announced entirely irrelevant. Let us not be distracted by the utilitarianists who, out of pity or some sympathy for this beautiful science, might quote all possible applications in engineering, astronomy, physics in the perfect illusion of making the discipline a big favour. I am not looking for excuses. (Besides, I've already remarked that mathematics is universally applicable in so far as we are willing to ignore the actual content of the specific objects -- and that should do.) Common sense will immediately agree on this irrelevance: mathematicians are strange people and it is hard to understand how they got that way.

Now, the fact that we cannot observe, nor even imagine, an independent existence of the ideal red does not mean that the red colour is irrelevant. Pure distinction, although not existing independently, may still have some relevance. If we saw in it merely a pure form of all experience all we could say here would be that it is of the fundamental philosophical importance. ``The apriori conditions of possibility of a possible experience in general are at the same time conditions of possibility of objects of experience.'' Experience without distinguishing is not possible. But rephrasing the quotation in our terms -- ``pure distinction is a basis of actual distinctions'' -- would only call the ghosts of One and Many to a new fight. Besides, the analogy with Kant does not go that far because we have admitted that pure distinction is not merely a condition of possibility of experience but is itself an experience. It is the experience underlying all experience and hence less (or not at all) relevant for any actual experience, but the more relevant for the experience of experiencing, for the experience of being.

The triviality of the statement that we distinguish is based on the fact that we, at the same time, experience distinguishing. And just as we experience pure distinction we experience its counterpart -- the lack of distinctions -- the One or nothingness. Along with any objects we see or vaguely feel their horizon, the background from which they emerge. In so far as it is the ultimate ground, the origin, it is One; but in so far as there is no distinction there, it is nothingness. We have the immediate experience of infinity -- of overflow, excess, things slipping out of our control, chaos or else, the experience of the objective existence of the world, of our own finitude. And finally, we can hardly imagine significance of an undifferentiated totality transcending our world of distinctions -- we rather picture infinity as the sum of all distinctions, as the differentiated analogy of the One. 

Pure distinction, multiplicity and infinity lie at the basis of our being in the world and, at the same time, constitute the origin of mathematical objects. This accounts for the almost mystical experience of reality in the middle of mathematical abstractions. ``The reality of mathematical experience'' is a witness of the still existing link between the technical constructions and the original intuitions. Various forms in which the basic notions occur in mathematics is perhaps useless as a justification of the philosophical statements but may nevertheless provide us with an inexhaustible source of inspiring analogies. Let me end by giving only one example. 

Stereographic projection is a very simple construction mapping in a one-to-one fashion an unlimited but finite sphere and the infinite plane. The figure gives the two-dimensional picture of this mapping.
\begin{center}
\input{ste}
\end{center}
Any point A on the plane is mapped to the point A' on the sphere which lies at the intersection between the sphere and the line drawn between the point A and the (upper) pole O of the sphere. Trivially -- and amazingly -- any two distinct points A and B are mapped to two distinct points A' and B' on the sphere, and vice versa. Except for the one point, the pole O -- the origin. The points infinitely far away are all mapped to O and O is the one (the One?) ``point in infinity''. One might imagine that points infinitely far to the left are different from those infinitely far to the right. But how can we distinguish between the points which are infinitely far away? Try to indicate a point and it will have to appear in a finite distance from the sphere. We cannot differentiate the infinite and so ``all points'' in infinity are indistinguishable; they are only the One ``point in infinity''. In the geometry on the sphere O plays exactly the same role as infinity plays in the geometry of the plane. Nothingness of the one point O is the finite image of the infinity which cannot be reflected by the distinctions on the finite sphere.

Since this is not an essay on God, being and nothingness, nor, I assure you, on the mystical truth of mathematics, I should probably stop here. So I do. \\[4ex]

\noindent {\bf References}\nopagebreak \\[1ex]\nopagebreak
\noindent [1]  G.Frege, Review of Husserl's Philosophie der Arithmetik, in 
{\em Translations from the Philosophical Writings of Gottlob Frege}, Basil Blackwell, Oxford, 1980. \\[1ex]
\noindent [2]  R.Carnap, The Logicist Foundation of Mathematics, in {\em Philosophy of Mathematics}, (second ed.), Cambridge University Press, 1983. \\[1ex]
\noindent [3]  P.Benacerraf, What Numbers Could Not Be, in {\em Philosophy of Mathematics}, (second ed.), Cambridge University Press, 1983.


\end{document}